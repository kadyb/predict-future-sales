---
title: "<center> **Predict Future Sales** <center>"
author: "<center> **Krzysztof Dyba** </center>"
date: "<center> `r Sys.Date()` </center>"
output:
  html_document: 
    toc: true
    toc_float: true
    highlight: tango
    theme: spacelab
    df_print: kable
---

<style>
body {text-align: justify}
</style>

# Introduction

The goal of this project is to predict the total amount of products sold in every shop in the future. Time-series dataset containing historical daily sales data was provided by **1C Company**, which is one of the largest Russian software firms.

## Files description
The following files were provided:

1. **sales_train.csv** - the training set. Daily historical data from January 2013 to October 2015:
    + **date** - date in format dd/mm/yyyy,
    + **date_block_num** - a consecutive month number (January 2013 is 0, October 2015 is 33),
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product,
    + **item_price** - current price of an item,
    + **item_cnt_day** - number of products sold,
2. **items.csv** - supplemental information about the items/products:
    + **item_name** - name of item,
    + **item_id** - unique identifier of a product,
    + **item_category_id** - unique identifier of item category,
3. **item_categories.csv** - supplemental information about the items categories:
    + **item_category_id** - unique identifier of item category,
    + **item_category_name** - name of item category,
4. **shops.csv** - supplemental information about the shops:
    + **shop_id** - unique identifier of a shop,
    + **shop_name** - name of shop,
5. **test.csv** - the test set for which a sales forecast should be made for the shops and products in November 2015:
    + **ID** - an ID that represents a (Shop, Item) tuple only in test set,
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product.

```{r include = FALSE}
startTime = Sys.time()
```

# Load libraries and data

**Load libraries**

```{r message = FALSE}
library("ggplot2")
library("cowplot")
library("data.table")
```

**Load data**

```{r message = FALSE}
sales = fread("data/sales_train_v2.csv", showProgress = FALSE)
items = fread("data/items.csv", encoding = "UTF-8")
shops = fread("data/shops.csv", encoding = "UTF-8")
itemCat = fread("data/item_categories.csv", encoding = "UTF-8")
testset = fread("data/test.csv", encoding = "UTF-8")
```

The **fread** function is used to read data, which is about 10 times faster than the standard **read.csv** function. In addition, it uses less memory and other functions are optimized for processing big data.

## Data conversion

```{r}
sales[, date := as.IDate(as.Date(date, format = "%d.%m.%Y"))]
```

Convert the date to the correct format (**"%Y-%m-%d"**) and type.

```{r}
sales = merge(sales, items, by = "item_id")
```

Merge items dataset with sales by **"item_id"** column.

```{r}
factorCols = c("item_id", "shop_id", "item_category_id")
sales[, (factorCols) := lapply(.SD, factor), .SDcols = factorCols]
# Skip the third column
testset[, (factorCols[-3]) := lapply(.SD, factor), .SDcols = factorCols[-3]]
```

Convert selected variables to factor data type.

# Exploratory data analysis

## First look {.tabset .tabset-fade}

### Dimension

```{r}
dim(sales)
```

The analyzed dataset consists of **2 935 849** rows and **8** columns.

### Groups count
```{r collapse = TRUE}
paste0("Shops: ", length(levels(sales[, shop_id])))
paste0("Items: ", length(levels(sales[, item_id])))
paste0("Items categories: ", length(levels(sales[, item_category_id])))
```

Sales data come from 60 stores, which sold 21 807 different items grouped into 84 categories.

### Missing data

```{r}
# The fastest way from tested to check NA in columns
apply(sapply(sales, is.na), 2, function(x) any(x))
```

There were no missing values in this dataset.

### Duplicates

```{r}
sales[duplicated(sales), ]
```

Only 6 duplicates were found, but none appear to be incorrect. They relate to the sale of computer games and movies.

### Sales

```{r}
ggplot(sales, aes(x = date_block_num)) +
  geom_bar() +
  geom_vline(xintercept = 11.5) +
  geom_vline(xintercept = 23.5) +
  annotate("text", x = 1.5, y = 150000, label = "2013") +
  annotate("text", x = 13.5, y = 150000, label = "2014") +
  annotate("text", x = 25.5, y = 150000, label = "2015") +
  labs(title = "Unique sales by month") +
  xlab("Month") +
  ylab("Sales") +
  theme_light()
```

The two months with the largest number of unique sales are December in 2014 and 2015 year. There is a downward trend over the months.

### Outliers

```{r fig.height = 2, fig.width = 8}
p1 = ggplot(sales, aes(y = item_price)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000),
                     labels = c("0", "100000", "200000", "300000")) +
  labs(title = "Prices of all products") +
  ylab("Price") +
  coord_flip() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

p2 = ggplot(sales, aes(y = item_cnt_day)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 500, 1000, 1500, 2000),
                     labels = c("0", "500", "1000", "1500", "2000")) +
  labs(title = "Daily sold products") +
  ylab("Sold products") +
  coord_flip() +
  theme_light() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# Function "plot_grid" is faster than "grid.arrange" (about 15 s)
plot_grid(p1, p2)
```

There are many outliers that need to be carefully analyzed and then correct or delete them.

**Prices of all products**

```{r}
head(sales[order(-item_price), ], 6)
```

The first item sold for 307 980 rubles is a remote control software sold in one transaction to 522 people, which results in such a high price. The original value was divided by 522.

The second outlier is the delivery service for a private online store. Comparing this value with the median price of this service (`r median(sales[item_id == 11365, item_price])`), it appears to be incorrect. This row has been removed from dataset.

```{r}
sales[item_price == 307980, item_price := 307980/522]
sales = sales[!(item_id == 11365 & item_price == 59200)]
```

The next values seem to be valid. They concern i.a. collector's edition of chess, specialized software or limited edition of consoles.

```{r}
sales[item_price == -1]
```

One negative value appears, which is probably also an error. The median value of other products with this ID is `r median(sales[item_id == 2973, item_price])`. This row has been removed.

```{r}
sales = sales[!(item_id == 2973 & item_price == -1)]
```

**Daily sold products**

```{r}
head(sales[order(-item_cnt_day), ], 5)
```

The largest number of transactions concerns the delivery service to the destination point. However, it is also probably incorrect. The cost of the service is significantly underestimated compared to others. This is well illustrated in the figure below.

```{r fig.height = 3, fig.width = 6}
cnt_df = data.frame(price = sales[item_id == 11373, item_price],
                    amount = sales[item_id == 11373, item_cnt_day])

ggplot(cnt_df, aes(x = price, y = amount, size = amount/price)) +
  geom_point(show.legend = FALSE) +
  labs(title = "Delivery service") +
  xlab("Amount") +
  ylab("Price") +
  theme_light()
```

```{r}
sales = sales[!(item_id == 11373 & item_cnt_day == 2169)]
```

The outlier mentioned above has been removed.

```{r}
head(sales[order(item_cnt_day), ], 5)
```

Negative values appear in the **"item_cnt_day"** column, which probably means product returns. In total, `r length(sales[item_cnt_day < 0, item_cnt_day])` returns were made and `r abs(sum(sales[item_cnt_day < 0, item_cnt_day]))` products were returned. These values have been preserved unchanged.

### Shop names

```{r}
shops[c(1, 2, 58, 59), 1]
```

These two locations in Yakutsk probably mean the same stores, although they have different IDs.

```{r}
shops[c(11, 12), 1]
```

A similar situation is in Zhukovsky. Store names differ only by one character ("?" instead of "2").

```{r collapse = TRUE}
sort(unique(sales[shop_id == 0, date_block_num]))
sort(unique(sales[shop_id == 57, date_block_num]))

sort(unique(sales[shop_id == 1, date_block_num]))
sort(unique(sales[shop_id == 58, date_block_num]))

sort(unique(sales[shop_id == 10, date_block_num]))
unique(sales[shop_id == 11, date_block_num])
```

Looking deeper into the data, it can be seen that the above stores contains missing months of sales from other stores, which confirms the hypothesis that they are the same stores. In this case, their ID was corrected in all datasets (in testset too).

```{r}
# First change ID, next drop unused factor
sales[shop_id == 0, shop_id := factor(57)]
sales[, shop_id := factor(shop_id)]
shops[shop_id == 0, shop_id := factor(57)]
shops[, shop_id := factor(shop_id)]
testset[shop_id == 0, shop_id := factor(57)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 1, shop_id := factor(58)]
sales[, shop_id := factor(shop_id)]
shops[shop_id == 1, shop_id := factor(58)]
shops[, shop_id := factor(shop_id)]
testset[shop_id == 1, shop_id := factor(58)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 11, shop_id := factor(10)]
sales[, shop_id := factor(shop_id)]
shops[shop_id == 11, shop_id := factor(10)]
shops[, shop_id := factor(shop_id)]
testset[shop_id == 11, shop_id := factor(10)]
testset[, shop_id := factor(shop_id)]
```

## Data mining

This subsection focuses on a more detailed analysis that will be useful for creating significant predictors, which should improve the efficiency of the pedication model.

```{r}
shop_id_grouped = sales[, .N, by = .(shop_id)]

ggplot(shop_id_grouped, aes(x = reorder(shop_id, N), y = N)) +
  geom_col() +
  labs(title = "Total sales by stores", caption = "3 stores have been merged") +
  xlab("Shop ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

Based on above chart, we can notice a strong unbalance in stores sales. The 5 stores with the largest total number of sales are located respectively in Moscow, Moscow, Khimki, Moscow and Yakutsk. However, the worst results are recorded in stores in Novosibirsk, astonishingly in Moscow and Voronezh.

## Check test set

In order to achieve the best results, it is necessary to analyze the data contained in the test set. This will allow to pay attention to the potential difference between the training and test set.

```{r collapse = TRUE}
checkShopSubset = unique(testset[, shop_id]) %in% unique(sales[, shop_id])

paste0("Shops: ", length(levels(testset[, shop_id])))
paste0("Are they a subset? ", all(checkShopSubset))
```

The test set contains 42 stores, which is 18 less than the training set. All test set stores appear in the training set.

```{r collapse = TRUE}
checkItemSubset = unique(testset[, item_id]) %in% unique(sales[, item_id])

paste0("Items: ", length(levels(testset[, item_id])))
paste0("Are they a subset? ", all(checkItemSubset))
```

The test set contains 5100 unique items, which is 16707 less than the training set. However, not all of them appeared in the training set.

```{r}
length(checkItemSubset[checkItemSubset == FALSE])
```

In November appeared 363 new items in stores.

# Feature engineer

1. Extract month and year from date to separate variables.

```{r}
sales[, month := as.factor(month(sales$date))]
sales[, year := as.factor(year(sales$date))]
```

2. Create a variable with the names of the places where the stores are located.

```{r}
shopLoc = c(rep("Yakutsk", 2), "Adygea", "Balashikha", "Volzhsky", "Vologda",
            rep("Voronezh", 3), "Outbound trade", rep("Zhukovsky", 2), 
            "Online store private", "Kazan", "Kazan", "Kaluga", "Kolomna", 
            rep("Krasnoyarsk", 2), "Kursk", rep("Moscow", 13), "Mytishchi", 
            rep("Nizhny Novgorod", 2), rep("Novosibirsk", 2), "Omsk", 
            rep("Rostov-on-Don", 3), rep("Saint Petersburg", 2), rep("Samara", 2), 
            "Sergiyev Posad", "Surgut", "Tomsk", rep("Tyumen", 3), "Ufa", "Ufa", 
            "Khimki", "Online store 1C", "Chekhov", rep("Yakutsk", 2), "Yaroslavl")
shopLoc = factor(shopLoc)
shopLabels = as.numeric(shopLoc)
```

3. Extract the main and secondary category names from the category description.

```{r collapse = TRUE}
# This is not the same as the column "item_category_id"
catNames = c("mainCategory", "subCategory")
itemCat[, (catNames) := tstrsplit(item_category_name, " - ", 
                                  type.convert = TRUE, fixed = TRUE)]
itemCat[is.na(subCategory), subCategory := mainCategory]

paste0("Main category: ", length(unique(itemCat[, mainCategory])))
paste0("Subcategory: ", length(unique(itemCat[, subCategory])))
```

4. Listing of national holidays and Sundays in Russia. 

```{r}
# Source: https://www.timeanddate.com/calendar/
holidays = data.table(date_block_num = seq(0, 35), 
                      holidays = c(10, 5, 6, 4, 9, 6, 4, 4, 5, 4, 5, 5, 
                                   10, 5, 7, 4, 9, 8, 4, 5, 4, 4, 8, 4, 
                                   12, 5, 6, 4, 9, 5, 4, 5, 4, 4, 6, 4))
```

5. Avarage exchange rate of dollar to ruble

```{r}
# Source: https://www.x-rates.com/average/?from=USD&to=RUB&amount=1
exchange = data.table(date_block_num = seq(0, 35), 
                      exchange = c(30.244, 30.180, 30.817, 31.347, 31.326, 32.309,
                                   32.769, 32.992, 32.601, 32.077, 32.688, 32.868,
                                   33.676, 35.245, 36.195, 35.659, 34.918, 34.392,
                                   34.685, 36.144, 37.951, 40.815, 46.257, 55.967,
                                   63.678, 64.443, 60.262, 53.179, 50.683, 54.611,
                                   57.156, 65.355, 66.950, 63.126, 65.083, 69.897))
```

# Data preparation

```{r}
testset[, date_block_num := 34]
```

# Summary

# TODO

TODO:

1. użyć xgboost lub ranger
2. kiedy produkt pojawil sie na rynku (miesiac jako num block)
3. ile czasu produkt byl na rynku (dni)
4. ile razy przedmiot byl zwracany (albo czy byl w ogole zwracany)
5. ile razy produkt zostal sprzedany w poprzednim miesiacu 
5. obliczyc iloczyn sprzedanych przedmiotow i ich ceny (ale co z tym zrobic, jesli nowe dane nie zawieraja ceny?)
8. dodac zmienna logiczna:
a) czy produkt byl w trainset
b) czy tuplet (sklep, produkt) byl w trainset
8. pamietac zeby przyciac wartosci do zakresu 0-20 (wieksza niz 20 staja sie 20)
9. *nie dodawac liczby dni w miesiecu ani dni gieldowych*

Inne modele:

1. ensemble modeling,
2. lstm,
3. hierarchical time series.


---

<center> <font size = "2"> <i>
The code was executed in `r round(difftime(Sys.time(), startTime, units = "mins"))` min.
<center> </font> </i>