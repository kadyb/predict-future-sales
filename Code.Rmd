---
title: "<center> **Predict Future Sales** <center>"
author: "<center> **Krzysztof Dyba** </center>"
date: "<center> `r Sys.Date()` </center>"
output:
  html_document: 
    toc: true
    toc_float: true
    highlight: tango
    theme: spacelab
    df_print: kable
    fig_width: 8
---

<style>
body {text-align: justify}
</style>

# Introduction

The goal of this project is to predict the total amount of products sold in every shop in the future. Time-series dataset containing historical daily sales data was provided by **1C Company**, which is one of the largest Russian software firms.

## Files description
The following files were provided:

1. **sales_train.csv** - the training set. Daily historical data from January 2013 to October 2015:
    + **date** - date in format dd/mm/yyyy,
    + **date_block_num** - a consecutive month number (January 2013 is 0, October 2015 is 33),
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product,
    + **item_price** - current price of an item,
    + **item_cnt_day** - number of products sold,
2. **items.csv** - supplemental information about the items/products:
    + **item_name** - name of item,
    + **item_id** - unique identifier of a product,
    + **item_category_id** - unique identifier of item category,
3. **item_categories.csv** - supplemental information about the items categories:
    + **item_category_id** - unique identifier of item category,
    + **item_category_name** - name of item category,
4. **shops.csv** - supplemental information about the shops:
    + **shop_id** - unique identifier of a shop,
    + **shop_name** - name of shop,
5. **test.csv** - the test set for which a sales forecast should be made for the shops and products in November 2015:
    + **ID** - an ID that represents a (Shop, Item) tuple only in test set,
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product.

```{r include = FALSE}
startTime = Sys.time()
```

# Load libraries and data

**Load libraries**

```{r message = FALSE}
library("ggplot2")
library("cowplot")
library("data.table")
```

```{r}
# Save session info
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
```

**Load data**

```{r message = FALSE}
sales = fread("data/sales_train_v2.csv", showProgress = FALSE)
items = fread("data/items.csv", encoding = "UTF-8")
shops = fread("data/shops.csv", encoding = "UTF-8")
itemCat = fread("data/item_categories.csv", encoding = "UTF-8")
testset = fread("data/test.csv")
```

The **fread** function is used to read data, which is about 10 times faster than the standard **read.csv** function. In addition, it uses less memory and other functions are optimized for processing big data.

## Data conversion

```{r}
sales[, date := as.IDate(as.Date(date, format = "%d.%m.%Y"))]
```

Convert the date to the correct format (**"%Y-%m-%d"**) and type.

```{r}
sales = merge(sales, items, by = "item_id")
```

Merge items dataset with sales by **"item_id"** column.

```{r}
factorCols = c("item_id", "shop_id", "item_category_id")
sales[, (factorCols) := lapply(.SD, factor), .SDcols = factorCols]
# Skip the third column
testset[, (factorCols[-3]) := lapply(.SD, factor), .SDcols = factorCols[-3]]
```

Convert selected variables to factor data type.

# Exploratory data analysis

## First look {.tabset .tabset-fade}

### Dimension

```{r}
dim(sales)
```

The analyzed dataset consists of **2 935 849** rows and **8** columns.

### Groups count
```{r collapse = TRUE}
paste0("Shops: ", length(levels(sales[, shop_id])))
paste0("Items: ", length(levels(sales[, item_id])))
paste0("Items categories: ", length(levels(sales[, item_category_id])))
```

Sales data come from 60 stores, which sold 21 807 different items grouped into 84 categories.

### Missing data

```{r}
# The fastest way from tested to check NA in columns
apply(sapply(sales, is.na), 2, function(x) any(x))
```

There were no missing values in this dataset.

### Duplicates

```{r}
sales[duplicated(sales), ]
```

Only 6 duplicates were found, but none appear to be incorrect. They relate to the sale of computer games and movies.

### Sales

```{r fig.height = 4}
salesMonth = sales[, .(.N), by = .(date_block_num)]

ggplot(salesMonth, aes(x = date_block_num, y = N)) +
  geom_col() +
  geom_vline(xintercept = 11.5) +
  geom_vline(xintercept = 23.5) +
  annotate("text", x = 1.5, y = 150000, label = "2013") +
  annotate("text", x = 13.5, y = 150000, label = "2014") +
  annotate("text", x = 25.5, y = 150000, label = "2015") +
  labs(title = "Unique sales by month",
       caption = "Labels on the X axis start from January 2013") +
  xlab("Month") +
  ylab("Sales") +
  theme_light()
```

The two months with the largest number of unique sales are December in 2014 and 2015 year. There is a downward trend over the months.

### Outliers

```{r fig.height = 2}
# Delete duplicated values, the distribution will be a little inaccurate, 
# but the execution time will decrease almost 30 times
salesPrice = sales[, .(item_price)]
salesPrice = salesPrice[!duplicated(salesPrice)]

# Add random duplicate values to make the distribution more close to real
salesItem = sales[, .(item_cnt_day)]
salesItemUnq = salesItem[!duplicated(item_cnt_day)]
set.seed(1)
salesItemDupl = salesItem[duplicated(salesItem)][sample(.N, 4000)]
salesItem = rbindlist(list(salesItemUnq, salesItemDupl))

p1 = ggplot(salesPrice, aes(y = item_price)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000),
                     labels = c("0", "100000", "200000", "300000")) +
  labs(title = "Prices of all products") +
  ylab("Price") +
  coord_flip() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

p2 = ggplot(salesItem, aes(y = item_cnt_day)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 500, 1000, 1500, 2000),
                     labels = c("0", "500", "1000", "1500", "2000")) +
  labs(title = "Daily sold products") +
  ylab("Sold products") +
  coord_flip() +
  theme_light() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# Function "plot_grid" is faster than "grid.arrange" (about 15 s)
plot_grid(p1, p2)
```

There are many outliers that need to be carefully analyzed and then correct or delete them.

#### **Prices of all products**

```{r}
head(sales[order(-item_price), ], 6)
```

The first item sold for 307 980 rubles is a remote control software sold in one transaction to 522 people, which results in such a high price. The original value was divided by 522.

The second outlier is the delivery service for a private online store. Comparing this value with the median price of this service (`r median(sales[item_id == 11365, item_price])`), it appears to be incorrect. This row has been removed from dataset.

```{r}
sales[item_price == 307980, item_price := 307980/522]
sales = sales[!(item_id == 11365 & item_price == 59200)]
```

The next values seem to be valid. They concern i.a. collector's edition of chess, specialized software or limited edition of consoles.

```{r}
sales[item_price == -1]
```

One negative value appears, which is probably also an error. The median value of other products with this ID is `r median(sales[item_id == 2973, item_price])`. This row has been removed.

```{r}
sales = sales[!(item_id == 2973 & item_price == -1)]
```

#### **Daily sold products**

```{r}
head(sales[order(-item_cnt_day), ], 5)
```

The largest number of transactions concerns the delivery service to the destination point. However, it is also probably incorrect. The cost of the service is significantly underestimated compared to others. This is well illustrated in the figure below.

```{r fig.height = 3}
cnt_df = data.frame(price = sales[item_id == 11373, item_price],
                    amount = sales[item_id == 11373, item_cnt_day])

ggplot(cnt_df, aes(x = price, y = amount, size = amount/price)) +
  geom_point(show.legend = FALSE) +
  labs(title = "Delivery service") +
  xlab("Amount") +
  ylab("Price") +
  theme_light()
```

```{r}
sales = sales[!(item_id == 11373 & item_cnt_day == 2169)]
```

The outlier mentioned above has been removed.

```{r}
head(sales[order(item_cnt_day), ], 5)
```

Negative values appear in the **"item_cnt_day"** column, which probably means product returns. In total, `r length(sales[item_cnt_day < 0, item_cnt_day])` returns were made and `r abs(sum(sales[item_cnt_day < 0, item_cnt_day]))` products were returned. These values have been preserved unchanged.

### Shop names

```{r}
shops[c(1, 2, 58, 59), 1]
```

These two locations in Yakutsk probably mean the same stores, although they have different IDs.

```{r}
shops[c(11, 12), 1]
```

A similar situation is in Zhukovsky. Store names differ only by one character ("?" instead of "2").

```{r collapse = TRUE}
sort(unique(sales[shop_id == 0, date_block_num]))
sort(unique(sales[shop_id == 57, date_block_num]))

sort(unique(sales[shop_id == 1, date_block_num]))
sort(unique(sales[shop_id == 58, date_block_num]))

sort(unique(sales[shop_id == 10, date_block_num]))
unique(sales[shop_id == 11, date_block_num])
```

Looking deeper into the data, it can be seen that the above stores contains missing months of sales from other stores, which confirms the hypothesis that they are the same stores. In this case, their ID was corrected in training and test set.

```{r}
# First change ID, next drop unused factor
# Note that "shops" datatable has not been corrected
sales[shop_id == 0, shop_id := factor(57)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 0, shop_id := factor(57)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 1, shop_id := factor(58)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 1, shop_id := factor(58)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 11, shop_id := factor(10)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 11, shop_id := factor(10)]
testset[, shop_id := factor(shop_id)]
```

## Data mining {.tabset .tabset-fade}

This subsection focuses on a more detailed analysis that will be useful for creating significant predictors, which should improve the efficiency of the pedication model.

### Unique items in categories

```{r}
catCounts = sales[, .(count = uniqueN(item_id)), by = .(item_category_id)]
catCounts = catCounts[order(-count)]

ggplot(catCounts, aes(count)) +
  geom_histogram(bins = 100) +
  geom_vline(xintercept = mean(catCounts[, count]), linetype = "solid") +
  geom_vline(xintercept = median(catCounts[, count]), linetype = "dashed") +
  labs(title = "Unique items in categories", 
       caption = "The solid line represents the mean
       The dashed line represents the median") +
  xlab("Items") +
  ylab("Frequency") +
  theme_light()
```

Category **cinema** (i.e. ID 40) contains over 4 964 unique items, while 10 categories contain less than 5 items. The median for all categories is 42 unique items and the average is around 260 unique items.

```{r}
catCounts[c(1:4, 81:84)]
```

### Total sales by stores

```{r fig.height = 7}
salesByShop = sales[, .(sum = sum(item_cnt_day)), by = .(shop_id)]

ggplot(salesByShop, aes(x = reorder(shop_id, sum), y = sum)) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000),
                     labels = c("0", "100000", "200000", "300000")) +
  labs(title = "Total sales by stores", caption = "3 stores have been merged") +
  xlab("Shop ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

Based on above chart, we can notice a strong unbalance in stores sales. The 5 stores with the largest total number of sales are located respectively in Moscow, Moscow, Khimki, Moscow and Yakutsk. However, the worst results are recorded in stores in Novosibirsk (360), Voronezh (3595) and Rostov on Don (4943).

### Total sales by categories

```{r}
salesByItemCat = sales[, .(sum = sum(item_cnt_day)), by = .(item_category_id)]

ggplot(salesByItemCat[1:30], aes(x = reorder(item_category_id, sum), y = sum)) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 200000, 400000),
                     labels = c("0", "200000", "400000")) +
  labs(title = "Total sales by categories",
       caption = "Only top 30 categories out of 84 are included") +
  xlab("Item category ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

Most items are sold in the **cinema** category, at the same time it is the most numerous category. The next two categories with the highest sales are **computer games** and **music**.

### Most sold items

```{r}
itemSumSales = sales[, .(sum = sum(item_cnt_day)), by = .(item_id)][order(-sum)]
itemSumSales = itemSumSales[1:50]

ggplot(itemSumSales[1:20], aes(x = reorder(item_id, sum), y = sum)) +
  geom_col() +
  labs(title = "Top 20 most sold items") +
  xlab("Item ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

The first most sold item was sold almost 11 times more often than the second item in the ranking. Let's check what this item is.

```{r}
head(sales[item_id == 20949, 7], 1)
```

It is a plastic bag.

```{r}
head(sales[item_id == 2808, 7], 1)
```

The second most sold item is a computer game.

### Price of items by categories

```{r fig.height = 6}
salesMeanSd = sales[, .(avg = mean(item_price), sd = sd(item_price)), 
                    by = .(item_category_id)]
salesMeanSd[is.na(sd), sd := 0]
salesMeanSd = salesMeanSd[order(-avg)]

ggplot(salesMeanSd[1:30], aes(x = reorder(item_category_id, avg), y = avg)) +
  geom_col() +
  geom_errorbar(aes(ymin = avg, ymax = avg + sd, color = "red"),
                show.legend = FALSE) +
  labs(title = "Price of items by categories", 
       caption = "The red line represents the standard deviation
       Only top 30 categories out of 84 are included") +
  xlab("Item category ID") +
  ylab("Average price") +
  coord_flip() +
  theme_light()
```

The highest average price is for the **game consoles** category, while the lowest is for the **gift** category. Most prices in categories are characterized by a large spread, the largest standard deviation can be noted for the **game consoles** category.

## Check test set

In order to achieve the best results, it is necessary to analyze the data contained in the test set. This will allow to pay attention to the potential difference between the training and test set.

```{r}
dim(testset)
```

The test set consists of **214 200** rows and **3** columns.

```{r collapse = TRUE}
checkShopSubset = unique(testset[, shop_id]) %in% unique(sales[, shop_id])

paste0("Shops: ", length(levels(testset[, shop_id])))
paste0("Are they a subset? ", all(checkShopSubset))
```

The test set contains 42 stores, which is 18 less than the training set. All test set stores appear in the training set.

```{r collapse = TRUE}
checkItemSubset = unique(testset[, item_id]) %in% unique(sales[, item_id])

paste0("Items: ", length(levels(testset[, item_id])))
paste0("Are they a subset? ", all(checkItemSubset))
```

The test set contains 5100 unique items, which is 16707 less than the training set. However, not all of them appeared in the training set.

```{r}
length(checkItemSubset[checkItemSubset == FALSE])
```

In November appeared 363 completely new items in stores.

```{r collapse = TRUE}
# Create character vector with unique combination of items and shops
# in training and test sets
pairShopItemTrain = paste(sales[, shop_id], sales[, item_id])
pairShopItemTrain = pairShopItemTrain[!duplicated(pairShopItemTrain)]
pairShopItemTest = paste(testset[, shop_id], sales[, item_id])
pairShopItemTest = pairShopItemTest[!duplicated(pairShopItemTest)]

# Check which pairs repeat in both sets
subsetShopItem = pairShopItemTest %in% pairShopItemTrain

paste0("Number of unique shop-item pairs: ", length(subsetShopItem))
paste0("Number of repeated pairs: ", sum(subsetShopItem))
paste0("Number of new pairs: ", length(subsetShopItem[subsetShopItem == FALSE]))
```

In addition to the appearance of completely new items, there is also the situation of selling new and old items in stores where they were not sold before. Out of 22 380 possible unique store-item combinations, there are 13 770 new items for sale in some shops. This means that we do not know what the sale of the item in these stores looked like before.

```{r include = FALSE}
endTime = Sys.time()
```

# Feature engineer

1. Extract month and year from date to separate variables.

```{r}
sales[, month := month(sales$date)]
sales[, year := year(sales$date)]
```

2. Create a variable with the names of the places where the stores are located.

```{r}
shopLoc = c(rep("Yakutsk", 2), "Adygea", "Balashikha", "Volzhsky", "Vologda",
            rep("Voronezh", 3), "Outbound trade", rep("Zhukovsky", 2), 
            "Online store private", "Kazan", "Kazan", "Kaluga", "Kolomna", 
            rep("Krasnoyarsk", 2), "Kursk", rep("Moscow", 13), "Mytishchi", 
            rep("Nizhny Novgorod", 2), rep("Novosibirsk", 2), "Omsk", 
            rep("Rostov-on-Don", 3), rep("Saint Petersburg", 2), rep("Samara", 2), 
            "Sergiyev Posad", "Surgut", "Tomsk", rep("Tyumen", 3), "Ufa", "Ufa", 
            "Khimki", "Online store 1C", "Chekhov", rep("Yakutsk", 2), "Yaroslavl")
shopLoc = factor(shopLoc)
shopLabels = as.numeric(shopLoc)
```

3. Extract the main and secondary category names from the category description.

```{r collapse = TRUE}
# This is not the same as the column "item_category_id"
catNames = c("mainCategory", "subCategory")
itemCat[, (catNames) := tstrsplit(item_category_name, " - ", 
                                  type.convert = TRUE, fixed = TRUE)]
itemCat[is.na(subCategory), subCategory := mainCategory]

paste0("Main category: ", length(unique(itemCat[, mainCategory])))
paste0("Subcategory: ", length(unique(itemCat[, subCategory])))
```

4. Listing of national holidays and Sundays in Russia. 

```{r}
# Source: https://www.timeanddate.com/calendar/
holidays = data.table(date_block_num = seq(0, 35), 
                      holidays = c(10, 5, 6, 4, 9, 6, 4, 4, 5, 4, 5, 5, 
                                   10, 5, 7, 4, 9, 8, 4, 5, 4, 4, 8, 4, 
                                   12, 5, 6, 4, 9, 5, 4, 5, 4, 4, 6, 4))
```

5. Avarage exchange rate of dollar to ruble

```{r}
# Source: https://www.x-rates.com/average/?from=USD&to=RUB&amount=1
exchange = data.table(date_block_num = seq(0, 35), 
                      exchange = c(30.244, 30.180, 30.817, 31.347, 31.326, 32.309,
                                   32.769, 32.992, 32.601, 32.077, 32.688, 32.868,
                                   33.676, 35.245, 36.195, 35.659, 34.918, 34.392,
                                   34.685, 36.144, 37.951, 40.815, 46.257, 55.967,
                                   63.678, 64.443, 60.262, 53.179, 50.683, 54.611,
                                   57.156, 65.355, 66.950, 63.126, 65.083, 69.897))
```

# Data preparation

```{r}
testset[, date_block_num := 34]
# rbindlist()
```

# Machine learning

# Summary

# TODO

TODO:

1. użyć xgboost lub ranger
2. kiedy produkt pojawil sie na rynku (miesiac jako num block)
3. ile czasu produkt byl na rynku (dni)
4. ile razy przedmiot byl zwracany (albo czy byl w ogole zwracany)
5. ile razy produkt zostal sprzedany w poprzednim miesiacu 
5. obliczyc iloczyn sprzedanych przedmiotow i ich ceny (ale co z tym zrobic, jesli nowe dane nie zawieraja ceny?)
- np. obliczyc mediane cen kategorii produktow i potem to jakos sklasyfikowac (np. cena niska, cena srednia, cena wysoka)
8. dodac zmienna logiczna:
a) czy produkt byl w trainset
b) czy tuplet (sklep, produkt) byl w trainset
9. pewnie bedzie trzeba dodac dane z testsetu do trainsetu i ustawic ich wartosci na 0
10. pamietac zeby przyciac wartosci do zakresu 0-20 (wieksza niz 20 staja sie 20)
11. *nie dodawac liczby dni w miesiecu ani dni gieldowych*

Co można jeszcze zrobić:

1. Unifikacja nazw przedmiotów (np. poprzez usunięcie zbędnych znaków czy wybranie dwóch pierwszych wyrazów) i wygenerowanie nowych ID. Wyszukanie podobieństw między nazwami (biblioteka "stringdist").

Inne modele:

1. ensemble modeling,
2. lstm,
3. hierarchical time series.


---

<center> <font size = "2"> <i>
Visualizations were made in `r round(difftime(endTime, startTime, units = "secs"))` s.

Data processing and machine learning were done in `r round(difftime(Sys.time(), endTime, units = "mins"))` min.
<center> </font> </i>