---
title: "<center> **Predict Future Sales** <center>"
author: "<center> **Krzysztof Dyba** </center>"
date: "<center> `r Sys.Date()` </center>"
output:
  html_document: 
    toc: true
    toc_float: true
    highlight: tango
    theme: spacelab
    df_print: kable
    fig_width: 8
---

<style>
body {text-align: justify}
</style>

# Introduction

The goal of this project is to predict the total amount of products sold in every shop in the future. Time-series dataset containing historical daily sales data was provided by **1C Company**, which is one of the largest Russian software firms.

## Files description
The following files were provided:

1. **sales_train.csv** - the training set. Daily historical data from January 2013 to October 2015:
    + **date** - date in format dd/mm/yyyy,
    + **date_block_num** - a consecutive month number (January 2013 is 0, October 2015 is 33),
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product,
    + **item_price** - current price of an item,
    + **item_cnt_day** - number of products sold,
2. **items.csv** - supplemental information about the items/products:
    + **item_name** - name of item,
    + **item_id** - unique identifier of a product,
    + **item_category_id** - unique identifier of item category,
3. **item_categories.csv** - supplemental information about the items categories:
    + **item_category_id** - unique identifier of item category,
    + **item_category_name** - name of item category,
4. **shops.csv** - supplemental information about the shops:
    + **shop_id** - unique identifier of a shop,
    + **shop_name** - name of shop,
5. **test.csv** - the test set for which a sales forecast should be made for the shops and products in November 2015:
    + **ID** - an ID that represents a (Shop, Item) tuple only in test set,
    + **shop_id** - unique identifier of a shop,
    + **item_id** - unique identifier of a product.

```{r include = FALSE}
startTime = Sys.time()
```

# Load libraries and data

**Load libraries**

```{r message = FALSE}
library("mlr3")
library("ggplot2")
library("cowplot")
library("paradox")
library("mlr3tuning")
library("data.table")
library("mlr3filters")
library("mlr3learners")
```

```{r}
# Save session info
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
```

**Load data**

```{r message = FALSE}
sales = fread("data/sales_train_v2.csv", showProgress = FALSE)
items = fread("data/items.csv", encoding = "UTF-8")
shops = fread("data/shops.csv", encoding = "UTF-8")
itemCat = fread("data/item_categories.csv", encoding = "UTF-8")
testset = fread("data/test.csv")
```

The **fread** function is used to read data, which is about 10 times faster than the standard **read.csv** function. In addition, it uses less memory and other functions are optimized for processing big data.

## Data conversion

```{r}
sales[, date := as.IDate(as.Date(date, format = "%d.%m.%Y"))]
```

Convert the date to the correct format (**"%Y-%m-%d"**) and type.

```{r}
sales = merge(sales, items, by = "item_id")
```

Merge items dataset with sales by **"item_id"** column.

```{r}
factorCols = c("item_id", "shop_id", "item_category_id")
sales[, (factorCols) := lapply(.SD, factor), .SDcols = factorCols]
# Skip the third column
testset[, (factorCols[-3]) := lapply(.SD, factor), .SDcols = factorCols[-3]]
```

Convert selected variables to factor data type.

# Exploratory data analysis

## First look {.tabset .tabset-fade}

### Dimension

```{r}
dim(sales)
```

The analyzed dataset consists of **2 935 849** rows and **8** columns.

### Groups count
```{r collapse = TRUE}
paste0("Shops: ", length(levels(sales[, shop_id])))
paste0("Items: ", length(levels(sales[, item_id])))
paste0("Items categories: ", length(levels(sales[, item_category_id])))
```

Sales data come from 60 stores, which sold 21 807 different items grouped into 84 categories.

### Missing data

```{r}
# The fastest way from tested to check NA in columns
apply(sapply(sales, is.na), 2, function(x) any(x))
```

There were no missing values in this dataset.

### Duplicates

```{r}
sales[duplicated(sales), ]
```

Only 6 duplicates were found, but none appear to be incorrect. They relate to the sale of computer games and movies.

### Sales

```{r fig.height = 4}
salesMonth = sales[, .(.N), by = .(date_block_num)]

ggplot(salesMonth, aes(x = date_block_num, y = N)) +
  geom_col() +
  geom_vline(xintercept = 11.5) +
  geom_vline(xintercept = 23.5) +
  annotate("text", x = 1.5, y = 150000, label = "2013") +
  annotate("text", x = 13.5, y = 150000, label = "2014") +
  annotate("text", x = 25.5, y = 150000, label = "2015") +
  labs(title = "Unique sales by month",
       caption = "Labels on the X axis start from January 2013") +
  xlab("Month") +
  ylab("Sales") +
  theme_light()
```

The two months with the largest number of unique sales are December in 2014 and 2015 year. There is a downward trend over the months.

### Outliers

```{r fig.height = 2}
# Delete duplicated values, the distribution will be a little inaccurate, 
# but the execution time will decrease almost 30 times
salesPrice = sales[, .(item_price)]
salesPrice = salesPrice[!duplicated(salesPrice)]

# Add random duplicate values to make the distribution more close to real
salesItem = sales[, .(item_cnt_day)]
salesItemUnq = salesItem[!duplicated(item_cnt_day)]
set.seed(1)
salesItemDupl = salesItem[duplicated(salesItem)][sample(.N, 4000)]
salesItem = rbindlist(list(salesItemUnq, salesItemDupl))

p1 = ggplot(salesPrice, aes(y = item_price)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000),
                     labels = c("0", "100000", "200000", "300000")) +
  labs(title = "Prices of all products") +
  ylab("Price") +
  coord_flip() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

p2 = ggplot(salesItem, aes(y = item_cnt_day)) +
  geom_boxplot() +
  scale_y_continuous(breaks = c(0, 500, 1000, 1500, 2000),
                     labels = c("0", "500", "1000", "1500", "2000")) +
  labs(title = "Daily sold products") +
  ylab("Sold products") +
  coord_flip() +
  theme_light() +
  theme_light() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

# Function "plot_grid" is faster than "grid.arrange" (about 15 s)
plot_grid(p1, p2)
```

There are many outliers that need to be carefully analyzed and then correct or delete them.

#### **Prices of all products**

```{r}
head(sales[order(-item_price), ], 6)
```

The first item sold for 307 980 rubles is a remote control software sold in one transaction to 522 people, which results in such a high price. The original value was divided by 522.

The second outlier is the delivery service for a private online store. Comparing this value with the median price of this service (`r median(sales[item_id == 11365, item_price])`), it appears to be incorrect. This row has been removed from dataset.

```{r}
sales[item_price == 307980, item_price := 307980/522]
sales = sales[!(item_id == 11365 & item_price == 59200)]
```

The next values seem to be valid. They concern i.a. collector's edition of chess, specialized software or limited edition of consoles.

```{r}
sales[item_price == -1]
```

One negative value appears, which is probably also an error. The median value of other products with this ID is `r median(sales[item_id == 2973, item_price])`. This row has been removed.

```{r}
sales = sales[!(item_id == 2973 & item_price == -1)]
```

#### **Daily sold products**

```{r}
head(sales[order(-item_cnt_day), ], 5)
```

The largest number of transactions concerns the delivery service to the destination point. However, it is also probably incorrect. The cost of the service is significantly underestimated compared to others. This is well illustrated in the figure below.

```{r fig.height = 3}
cnt_df = data.frame(price = sales[item_id == 11373, item_price],
                    amount = sales[item_id == 11373, item_cnt_day])

ggplot(cnt_df, aes(x = price, y = amount, size = amount/price)) +
  geom_point(show.legend = FALSE) +
  labs(title = "Delivery service") +
  xlab("Amount") +
  ylab("Price") +
  theme_light()
```

```{r}
sales = sales[!(item_id == 11373 & item_cnt_day == 2169)]
```

The outlier mentioned above has been removed.

```{r}
head(sales[order(item_cnt_day), ], 5)
```

Negative values appear in the **"item_cnt_day"** column, which probably means product returns. In total, `r length(sales[item_cnt_day < 0, item_cnt_day])` returns were made and `r abs(sum(sales[item_cnt_day < 0, item_cnt_day]))` products were returned. These values have been preserved unchanged.

### Shop names

```{r}
shops[c(1, 2, 58, 59), 1]
```

These two locations in Yakutsk probably mean the same stores, although they have different IDs.

```{r}
shops[c(11, 12), 1]
```

A similar situation is in Zhukovsky. Store names differ only by one character ("?" instead of "2").

```{r collapse = TRUE}
sort(unique(sales[shop_id == 0, date_block_num]))
sort(unique(sales[shop_id == 57, date_block_num]))

sort(unique(sales[shop_id == 1, date_block_num]))
sort(unique(sales[shop_id == 58, date_block_num]))

sort(unique(sales[shop_id == 10, date_block_num]))
unique(sales[shop_id == 11, date_block_num])
```

Looking deeper into the data, it can be seen that the above stores contains missing months of sales from other stores, which confirms the hypothesis that they are the same stores. In this case, their ID was corrected in training and test set.

```{r}
# First change ID, next drop unused factor
# Note that "shops" datatable has not been corrected
sales[shop_id == 0, shop_id := factor(57)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 0, shop_id := factor(57)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 1, shop_id := factor(58)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 1, shop_id := factor(58)]
testset[, shop_id := factor(shop_id)]

sales[shop_id == 11, shop_id := factor(10)]
sales[, shop_id := factor(shop_id)]
testset[shop_id == 11, shop_id := factor(10)]
testset[, shop_id := factor(shop_id)]
```

## Data mining {.tabset .tabset-fade}

This subsection focuses on a more detailed analysis that will be useful for creating significant predictors, which should improve the efficiency of the predictive model.

### Unique items in categories

```{r}
catCounts = sales[, .(count = uniqueN(item_id)), by = .(item_category_id)]
catCounts = catCounts[order(-count)]

ggplot(catCounts, aes(count)) +
  geom_histogram(bins = 100) +
  geom_vline(xintercept = mean(catCounts[, count]), linetype = "solid") +
  geom_vline(xintercept = median(catCounts[, count]), linetype = "dashed") +
  labs(title = "Unique items in categories", 
       caption = "The solid line represents the mean
       The dashed line represents the median") +
  xlab("Items") +
  ylab("Frequency") +
  theme_light()
```

Category **cinema** (i.e. ID 40) contains over 4 964 unique items, while 10 categories contain less than 5 items. The median for all categories is 42 unique items and the average is around 260 unique items.

```{r}
catCounts[c(1:4, 81:84)]
```

### Total sales by stores

```{r fig.height = 7}
salesByShop = sales[, .(sum = sum(item_cnt_day)), by = .(shop_id)]

ggplot(salesByShop, aes(x = reorder(shop_id, sum), y = sum)) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000),
                     labels = c("0", "100000", "200000", "300000")) +
  labs(title = "Total sales by stores", caption = "3 stores have been merged") +
  xlab("Shop ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

Based on above chart, we can notice a strong unbalance in stores sales. The 5 stores with the largest total number of sales are located respectively in Moscow, Moscow, Khimki, Moscow and Yakutsk. However, the worst results are recorded in stores in Novosibirsk (360), Voronezh (3595) and Rostov on Don (4943).

### Total sales by categories

```{r}
salesByItemCat = sales[, .(sum = sum(item_cnt_day)), by = .(item_category_id)]

ggplot(salesByItemCat[1:30], aes(x = reorder(item_category_id, sum), y = sum)) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 200000, 400000),
                     labels = c("0", "200000", "400000")) +
  labs(title = "Total sales by categories",
       caption = "Only top 30 categories out of 84 are included") +
  xlab("Item category ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

Most items are sold in the **cinema** category, at the same time it is the most numerous category. The next two categories with the highest sales are **computer games** and **music**.

### Most sold items

```{r}
itemSumSales = sales[, .(sum = sum(item_cnt_day)), by = .(item_id)][order(-sum)]
itemSumSales = itemSumSales[1:50]

ggplot(itemSumSales[1:20], aes(x = reorder(item_id, sum), y = sum)) +
  geom_col() +
  labs(title = "Top 20 most sold items") +
  xlab("Item ID") +
  ylab("Sales") +
  coord_flip() +
  theme_light()
```

The first most sold item was sold almost 11 times more often than the second item in the ranking. Let's check what this item is.

```{r}
head(sales[item_id == 20949, 7], 1)
```

It is a plastic bag.

```{r}
head(sales[item_id == 2808, 7], 1)
```

The second most sold item is a computer game.

### Price of items by categories

```{r fig.height = 6}
salesMeanSd = sales[, .(avg = mean(item_price), sd = sd(item_price)), 
                    by = .(item_category_id)]
salesMeanSd[is.na(sd), sd := 0]
salesMeanSd = salesMeanSd[order(-avg)]

ggplot(salesMeanSd[1:30], aes(x = reorder(item_category_id, avg), y = avg)) +
  geom_col() +
  geom_errorbar(aes(ymin = avg, ymax = avg + sd, color = "red"),
                show.legend = FALSE) +
  labs(title = "Price of items by categories", 
       caption = "The red line represents the standard deviation
       Only top 30 categories out of 84 are included") +
  xlab("Item category ID") +
  ylab("Average price") +
  coord_flip() +
  theme_light()
```

The highest average price is for the **game consoles** category, while the lowest is for the **gift** category. Most prices in categories are characterized by a large spread, the largest standard deviation can be noted for the **game consoles** category.

### Data relations
```{r}
# catCounts - number of unique items in categories
# salesByItemCat - sum of sold items by categories
# salesMeanSd[, -3] - avarage price of sold items by categories

dataRelationsCat = Reduce(merge, list(catCounts, salesByItemCat, salesMeanSd[, -3]))

p1 = ggplot(dataRelationsCat, aes(log(count), log(sum))) +
  geom_point() +
  geom_smooth(method = lm) +
  xlab("Number of unique items [log]") +
  ylab("Sum of sold items [log]") +
  theme_light()
  
p2 = ggplot(dataRelationsCat, aes(log(count), log(avg))) +
  geom_point() +
  xlab("Number of unique items [log]") +
  ylab("Avarage price of sold items [log]") +
  theme_light()

p3 = ggplot(dataRelationsCat, aes(log(avg), log(sum))) +
  geom_point() +
  xlab("Avarage price of sold items [log]") +
  ylab("Sum of sold items [log]") +
  theme_light()

pGrid = plot_grid(p1, p2, p3, ncol = 3)
pTitle = ggdraw() + draw_label("Data aggregated by categories")
plot_grid(pTitle, pGrid, ncol = 1, rel_heights = c(0.1, 1))
```

You can notice the relationship between variables of total items sales and the number of unique items offered in the categories. There were no statistically significant relationships for other variables.

```{r}
# The tau coefficient was used for testing because the distributions of the analyzed
# variables were not close to the normal distribution and contained repeated values.
cor.test(dataRelationsCat[, count], dataRelationsCat[, sum], 
         method = "kendall")$estimate
```

The value of rank relationship for this case is about 0.55.

```{r}
catCountsShop = sales[, .(count = uniqueN(item_id)), by = .(shop_id)]
# salesByShop - sum of sold items by shops
salesMeanShop = sales[, .(avg = mean(item_price)), by = .(shop_id)]

dataRelationsShop = Reduce(merge, list(catCountsShop, salesByShop, salesMeanShop))

p1 = ggplot(dataRelationsShop, aes(count, sqrt(sum))) +
  geom_point() +
  geom_smooth(method = lm) +
  xlab("Number of unique items [n]") +
  ylab("Sum of sold items [sqrt]") +
  theme_light()
  
p2 = ggplot(dataRelationsShop, aes(count, avg)) +
  geom_point() +
  xlab("Number of unique items [n]") +
  ylab("Avarage price of sold items [n]") +
  theme_light()

p3 = ggplot(dataRelationsShop, aes(avg, sqrt(sum))) +
  geom_point() +
  xlab("Avarage price of sold items [n]") +
  ylab("Sum of sold items [sqrt]") +
  theme_light()

pGrid = plot_grid(p1, p2, p3, ncol = 3)
pTitle = ggdraw() + draw_label("Data aggregated by shops")
plot_grid(pTitle, pGrid, ncol = 1, rel_heights = c(0.1, 1))
```

This relationship looks even better when aggregated to stores.

```{r}
cor.test(dataRelationsShop[, count], dataRelationsShop[, sum], 
         method = "kendall")$estimate
```

The rank correlation value in this case increased up to 0.77. Generally, this means that the more different items a store offers for sale, the more total sales will be achieved.

## Check test set

In order to achieve the best results, it is necessary to analyze the data contained in the test set. This will allow to pay attention to the potential difference between the training and test set.

```{r}
dim(testset)
```

The test set consists of **214 200** rows and **3** columns.

```{r collapse = TRUE}
checkShopSubset = unique(testset[, shop_id]) %in% unique(sales[, shop_id])

paste0("Shops: ", length(levels(testset[, shop_id])))
paste0("Are they a subset? ", all(checkShopSubset))
```

The test set contains 42 stores, which is 18 less than the training set. All test set stores appear in the training set.

```{r collapse = TRUE}
checkItemSubset = unique(testset[, item_id]) %in% unique(sales[, item_id])

paste0("Items: ", length(levels(testset[, item_id])))
paste0("Are they a subset? ", all(checkItemSubset))
```

The test set contains 5100 unique items, which is 16707 less than the training set. However, not all of them appeared in the training set.

```{r}
length(checkItemSubset[checkItemSubset == FALSE])
```

In November appeared 363 completely new items in stores.

```{r collapse = TRUE}
# Create character vector with unique combination of items and shops
# in training and test sets
pairShopItemTrain = paste(sales[, shop_id], sales[, item_id])
pairShopItemTrain = pairShopItemTrain[!duplicated(pairShopItemTrain)]
pairShopItemTest = paste(testset[, shop_id], sales[, item_id])
pairShopItemTest = pairShopItemTest[!duplicated(pairShopItemTest)]

# Check which pairs repeat in both sets
subsetShopItem = pairShopItemTest %in% pairShopItemTrain

paste0("Number of unique shop-item pairs: ", length(subsetShopItem))
paste0("Number of repeated pairs: ", sum(subsetShopItem))
paste0("Number of new pairs: ", length(subsetShopItem[subsetShopItem == FALSE]))
```

In addition to the appearance of completely new items, there is also the situation of selling new and old items in stores where they were not sold before. Out of 22 380 possible unique store-item combinations, there are 13 770 new items for sale in some shops. This means that we do not know what the sale of the item in these stores looked like before.

```{r include = FALSE}
endTime = Sys.time()
```

# Feature engineer {.tabset .tabset-fade}

## Feature engineer

1. Create vectors with months and years.

```{r}
month = rep(1:12, 3)
year = rep(2013:2015, each = 12)
dates = data.table(date_block_num = seq(0, 35), month, year)
```

2. Create a variable with the names of the places where the stores are located.

```{r}
shopLoc = c(rep("Yakutsk", 2), "Adygea", "Balashikha", "Volzhsky", "Vologda",
            rep("Voronezh", 3), "Outbound trade", rep("Zhukovsky", 2), 
            "Online store private", "Kazan", "Kazan", "Kaluga", "Kolomna", 
            rep("Krasnoyarsk", 2), "Kursk", rep("Moscow", 13), "Mytishchi", 
            rep("Nizhny Novgorod", 2), rep("Novosibirsk", 2), "Omsk", 
            rep("Rostov-on-Don", 3), rep("Saint Petersburg", 2), rep("Samara", 2), 
            "Sergiyev Posad", "Surgut", "Tomsk", rep("Tyumen", 3), "Ufa", "Ufa", 
            "Khimki", "Online store 1C", "Chekhov", rep("Yakutsk", 2), "Yaroslavl")
shopLoc = factor(shopLoc)
shopLabels = data.table(shop_id = shops$shop_id, 
                        shopLabel = as.numeric(shopLoc))
```

3. Extract the main and secondary category names from the category description.

```{r collapse = TRUE}
# This is not the same as the column "item_category_id" (n = 84)
catNames = c("mainCategory", "subCategory")
itemCat[, (catNames) := tstrsplit(item_category_name, " - ", 
                                  type.convert = TRUE, fixed = TRUE)]
itemCat[is.na(subCategory), subCategory := mainCategory]
factorCols = c("mainCategory", "subCategory")
itemCat[, (factorCols) := lapply(.SD, factor), .SDcols = factorCols]
itemCat[, mainCategoryID := as.numeric(mainCategory)]
itemCat[, subCategoryID := as.numeric(subCategory)]
itemCat = merge(itemCat, items[, -1], by = "item_category_id")

paste0("Main category: ", length(unique(itemCat[, mainCategory])))
paste0("Subcategory: ", length(unique(itemCat[, subCategory])))
```

4. Listing of national holidays and Sundays in Russia. 

```{r}
# Source: https://www.timeanddate.com/calendar/
holidays = data.table(date_block_num = seq(0, 35), 
                      holidays = c(10, 5, 6, 4, 9, 6, 4, 4, 5, 4, 5, 5, 
                                   10, 5, 7, 4, 9, 8, 4, 5, 4, 4, 8, 4, 
                                   12, 5, 6, 4, 9, 5, 4, 5, 4, 4, 6, 4))
```

5. Average exchange rate of dollar to ruble.

```{r}
# Source: https://www.x-rates.com/average/?from=USD&to=RUB&amount=1
exchange = data.table(date_block_num = seq(0, 35), 
                      exchange = c(30.244, 30.180, 30.817, 31.347, 31.326, 32.309,
                                   32.769, 32.992, 32.601, 32.077, 32.688, 32.868,
                                   33.676, 35.245, 36.195, 35.659, 34.918, 34.392,
                                   34.685, 36.144, 37.951, 40.815, 46.257, 55.967,
                                   63.678, 64.443, 60.262, 53.179, 50.683, 54.611,
                                   57.156, 65.355, 66.950, 63.126, 65.083, 69.897))
```

6. Unification of product names and generating new IDs.

```{r}
items$cor_items = tolower(items$item_name)
items$cor_items = sapply(strsplit(items$cor_items, "\\("), `[`, 1)
items$cor_items = sapply(strsplit(items$cor_items, "\\["), `[`, 1)
items$cor_items = gsub("[^A-Za-z0-9А-Яа-я]+", " ", items$cor_items)
items$cor_items = trimws(items$cor_items)
items$cor_item_id = as.integer(as.factor(items$cor_items))
```

```{r}
# Convert selected variables to numeric data type
# Don't use default as.numeric function because loss of information (drop some levels)
as.numeric.factor = function(x) {as.numeric(levels(x))[x]}
intCols = c("shop_id", "item_id")
sales[, (intCols) := lapply(.SD, as.numeric.factor), .SDcols = intCols]
```

7. The month in which the product first appeared on the market and in specific stores.

```{r}
firstAppear = sales[, .(firstAppear = min(date_block_num)), by = .(item_id)]
firstAppearShop = sales[, .(firstAppearShop = min(date_block_num)), 
                        by = .(item_id, shop_id)]
```

8. The number of months the product has been sold generally and in specific stores.

```{r}
salesDur = sales[, .(salesDur = max(date_block_num) - min(date_block_num)),
                 by = .(item_id)]
salesDurShop = sales[, .(salesDurShop = max(date_block_num) - min(date_block_num)),
                     by = .(item_id, shop_id)]
```

9. The number of unique items in the shops.

```{r}
numItemShop = sales[, .(numItemShop = uniqueN(item_id)), by = .(shop_id)]
```

10. Item sales in previous months by shop.

```{r}
salesLag = sales[, .(item_cnt_month = sum(item_cnt_day)),
                 by = .(shop_id, item_id, date_block_num)]

testsetLag = testset[, -1]
testsetLag[, (intCols) := lapply(.SD, as.numeric.factor), .SDcols = intCols]
testsetLag[, date_block_num := 34]
testsetLag[, item_cnt_month := NA]
# merge shops and items from trainset and testset
salesLag = rbindlist(list(salesLag, testsetLag))

# add "cor_item_id"
salesLag = merge(salesLag, items[, c(2, 5)], by = "item_id")

salesLagFun = function(dt, item_col, lag) {
  colname = paste0("sales_lag_", lag, "_", item_col)
  if(item_col == "item_id") {
    dt = dt[dt[, .(shop_id, item_id, item_cnt_month,
                                   date_block_num = date_block_num + lag)],
                      on = c("shop_id", "item_id", "date_block_num"),
                      (colname) := i.item_cnt_month]
  } 
  else if (item_col == "cor_item_id") {
      dt = dt[dt[, .(shop_id, cor_item_id, item_cnt_month,
                                   date_block_num = date_block_num + lag)],
                      on = c("shop_id", "cor_item_id", "date_block_num"),
                      (colname) := i.item_cnt_month]
  }
  else stop("Invalid item_col name.")
  
  dt[is.na(get(colname)), (colname) := 0]
  dt[get(colname) < 0, (colname) := 0]
  dt[get(colname) > 20, (colname) := 20] 
  return(dt)
}

salesLag = salesLagFun(salesLag, "item_id", 1)
salesLag = salesLagFun(salesLag, "item_id", 2)
salesLag = salesLagFun(salesLag, "cor_item_id", 1)
salesLag = salesLag[, -"item_cnt_month"]
```

11. How many times the item was returned to the store.

```{r}
wasReturned = sales[item_cnt_day < 0, item_id]
nReturns = data.table(item_id = wasReturned, wasReturned = 1)
nReturns = nReturns[, .(item_returns = sum(wasReturned)), by = .(item_id)]
```

12. Average price of items by categories for individual months and their standard deviation.
```{r}
groupPrices = sales[, .(itemCategoryPriceMean = mean(item_price),
                        itemCategoryPriceSD = sd(item_price)),
                 by = .(item_category_id, date_block_num)]
groupPrices[is.na(itemCategoryPriceSD), itemCategoryPriceSD := 0]
groupPrices$item_category_id = as.numeric.factor(groupPrices$item_category_id)

# data for month 34 will be taken from last month in groupPrices
lastMonth = groupPrices[, .(lastMonth = max(date_block_num)), 
                        by = .(item_category_id)]
lastMonth = merge(lastMonth, groupPrices, 
                  by.x = c("item_category_id", "lastMonth"), 
                  by.y = c("item_category_id", "date_block_num"))
lastMonth = lastMonth[, -"lastMonth"]
lastMonth$date_block_num = 34
groupPrices = rbindlist(list(groupPrices, lastMonth), use.names = TRUE)
```

13. Add new items from the test set to training data and set their sales value to 0. In addition, mark which item is new.

```{r}
newItems = unique(testset$item_id)[!checkItemSubset]
newItems = as.numeric.factor(newItems)
newItems = CJ(item_id = newItems, 
              shop_id = unique(sales$shop_id), 
              date_block_num = 0:33)
newItems[, item_cnt_month := 0]
newItems[, newItem := 1]
```

```{r eval = FALSE, include = FALSE}
# pairs consisting of the item and store which didn't appear in the trainset
newItemPairs = data.table(pair = pairShopItemTest[!subsetShopItem])
newItemPairs = CJ(pair = newItemPairs$pair, date_block_num = 0:33)
newItemPairs[, c("shop_id", "item_id") := tstrsplit(pair, " ", fixed = TRUE)]
newItemPairs = newItemPairs[, -"pair"]
newItemPairs[, (intCols) := lapply(.SD, as.integer), .SDcols = intCols]
newItemPairs[, newItemShop := 1]
```

## Data preparation

```{r}
salesPrepared = sales[, .(item_cnt_month = sum(item_cnt_day)),
                      by = .(shop_id, item_id, date_block_num)]
salesPrepared[item_cnt_month < 0, "item_cnt_month"] = 0
salesPrepared[item_cnt_month > 20, "item_cnt_month"] = 20
salesPrepared[, newItem := 0]

# add new items from testset to trainset
salesPrepared = rbindlist(list(salesPrepared, newItems), use.names = TRUE)
```

Aggregate input data by month to main datatable and cut to the required range of values.

```{r}
salesPrepared = merge(salesPrepared, dates, by = "date_block_num")
salesPrepared = merge(salesPrepared, shopLabels, by = "shop_id")
salesPrepared = merge(salesPrepared, itemCat[, c(1, 5:7)], by = "item_id")
salesPrepared = merge(salesPrepared, holidays, by = "date_block_num")
salesPrepared = merge(salesPrepared, exchange, by = "date_block_num")
salesPrepared = merge(salesPrepared, items[, c(2, 5)], by = "item_id")
salesPrepared = merge(salesPrepared, firstAppear, by = "item_id", all.x = TRUE)
salesPrepared = merge(salesPrepared, firstAppearShop, by = c("item_id", "shop_id"), all.x = TRUE)
salesPrepared = merge(salesPrepared, salesDur, by = "item_id", all.x = TRUE)
salesPrepared = merge(salesPrepared, salesDurShop, by = c("item_id", "shop_id"), all.x = TRUE)
salesPrepared = merge(salesPrepared, numItemShop, by = c("shop_id"))
salesPrepared = merge(salesPrepared, salesLag[, -4], by = c("item_id", "shop_id", "date_block_num"), all.x = TRUE)
salesPrepared = merge(salesPrepared, nReturns, by = "item_id", all.x = TRUE)
salesPrepared = merge(salesPrepared, groupPrices, by = c("item_category_id", "date_block_num"))

# fill NAs
salesPrepared[is.na(firstAppearShop), firstAppearShop := -1]
salesPrepared[is.na(firstAppear), firstAppear := -1]
salesPrepared[is.na(salesDurShop), salesDurShop := -1]
salesPrepared[is.na(salesDur), salesDur := -1]
salesPrepared[is.na(sales_lag_1_item_id), sales_lag_1_item_id := 0]
salesPrepared[is.na(sales_lag_2_item_id), sales_lag_2_item_id := 0]
salesPrepared[is.na(sales_lag_1_cor_item_id), sales_lag_1_cor_item_id := 0]
salesPrepared[is.na(item_returns), item_returns := 0]
```

Merge main datatable with created explanatory variables. Finally, the data set consists of **`r nrow(salesPrepared)`** rows and **`r ncol(salesPrepared)`** columns.

```{r}
testset[, (intCols) := lapply(.SD, as.numeric.factor), .SDcols = intCols]
testset[, date_block_num := 34]
testset[, month := 11]
testset[, year := 2015]
testset = merge(testset, shopLabels, by = "shop_id")
testset = merge(testset, itemCat[, c(1, 5:7)], by = "item_id")
testset = merge(testset, holidays, by = "date_block_num")
testset = merge(testset, exchange, by = "date_block_num")
testset = merge(testset, items[, c(2, 5)], by = "item_id")

# Fill NAs for items and (items, shop) which don't appear in sales datatable
testset = merge(testset, firstAppear, by = "item_id", all.x = TRUE)
testset[is.na(testset$firstAppear), "firstAppear"] = -1
testset = merge(testset, firstAppearShop, by = c("item_id", "shop_id"), all.x = TRUE)
testset[is.na(testset$firstAppearShop), "firstAppearShop"] = -1

# Fill NAs for items and (items, shop) which don't appear in sales datatable
# If the item was still sold in November, add one more month
testset = merge(testset, salesDur, by = "item_id", all.x = TRUE)
testset[, salesDur := as.integer(salesDur + 1)]
testset[is.na(testset$salesDur), "salesDur"] = -1
testset = merge(testset, salesDurShop, by = c("item_id", "shop_id"), all.x = TRUE)
testset[, salesDurShop := as.integer(salesDurShop + 1)]
testset[is.na(testset$salesDurShop), "salesDurShop"] = -1

testset = merge(testset, numItemShop, by = c("shop_id"))
testset = merge(testset, salesLag[, -4], by = c("item_id", "shop_id", "date_block_num"))
testset = merge(testset, nReturns, by = "item_id", all.x = TRUE)
testset[is.na(item_returns), item_returns := 0]
testset = merge(testset, groupPrices, by = c("item_category_id", "date_block_num"))
testset$newItem = fifelse(testset$item_id %in% unique(newItems$item_id), 1, 0)
```

Prepare testset.

# Machine learning {.tabset .tabset-fade}

## Model training

```{r include = FALSE}
set.seed(1)

task = TaskRegr$new("task", backend = salesPrepared, target = "item_cnt_month")

lrn_xgb = mlr_learners$get("regr.xgboost")

ps = ParamSet$new(
  params = list(
    ParamDbl$new(id = "eta", lower = 0.1, upper = 0.3),
    ParamDbl$new(id = "gamma", lower = 0, upper = 100),
    ParamInt$new(id = "max_depth", lower = 6, upper = 10),
    ParamDbl$new(id = "min_child_weight", lower = 1, upper = 300),
    ParamDbl$new(id = "subsample", lower = 0.5, upper = 1),
    ParamDbl$new(id = "colsample_bytree",  lower = 0.7, upper = 1),
    ParamDbl$new(id = "colsample_bylevel", lower = 0.7, upper = 1),
    ParamDbl$new(id = "colsample_bynode", lower = 0.7, upper = 1),
    ParamInt$new(id = "nrounds", lower = 1, upper = 1000)
))

at = AutoTuner$new(learner = lrn_xgb, 
                   resampling = rsmp("cv", folds = 3),
                   measures = msr("regr.rmse"), 
                   tune_ps = ps,
                   terminator = term("evals", n_evals = 40),
                   tuner = tnr("random_search"))

resampling_outer = rsmp("cv", folds = 3)
rr = resample(task = task, learner = at, resampling = resampling_outer)

at$train(task)
```

## Evaluation

```{r}
# selected hyperparametrs
as.data.table(at$tuning_result$tune_x)
```

The lowest RMSE value of `r round(at$tuning_result$perf, 2)` was obtained for the above parameters.

```{r fig.height = 3}
# inner cv error
innerError = data.table(value = at$tuning_instance$archive()$regr.rmse,
                      cv = "inner")
# outer cv error
outerError = data.table(value = rr$score(msr("regr.rmse"))$regr.rmse,
                       cv = "outer") 

cvError = rbind(innerError, outerError)

ggplot(cvError) +
  geom_boxplot(aes(x = cv, y = value)) +
  xlab("Crossvalidation") +
  ylab("RMSE") +
  labs(title = "Model evaluation") +
  scale_x_discrete(labels = c(paste("Inner\ni = ", nrow(innerError)), 
                               paste("Outer\ni = ", nrow(outerError)))) +
  theme_light()
```

The median of RMSE for inner crossvalidation was `r round(median(innerError$value), 2)`, while for outer crossvalidation was `r round(median(outerError$value), 2)`.

## Variable importance

```{r}
filter = flt("importance", learner = lrn_xgb)
filter$calculate(task)

ggplot(as.data.table(filter), aes(x = reorder(feature, score), y = score)) +
   geom_col() +
   xlab("Variable") +
   ylab("Importance") +
   labs(title = "Variable importance") +
   coord_flip() +
   theme_light()
```

## Output data

```{r}
testset = testset[order(ID)]
testset = testset[, -"ID"]
intCols = c("date_block_num", "month", "year")
testset[, (intCols) := lapply(.SD, as.integer), .SDcols = intCols]
results = as.data.table(at$predict_newdata(testset, task))$response
results[results < 0] = 0
results[results > 20] = 20
outputfile = data.table(ID = 0:214199, item_cnt_month = results)
write.csv(outputfile, "results.csv", row.names = FALSE, quote = FALSE)
```

Preparing the output file with results for the evaluation.

# Summary
In this project, an analysis of past sales was performed and a predictive model of future sales of items in stores was developed based on historical data. Data mining allowed to choose the appropriate explanatory variables in the feature engineer process. Finally, an RMSE value of 1.47 was reached on the external test set using gradient boosting algorithm. The results obtained are completely reproducible including automatic data processing and modeling.

#### **Remarks**

The most important conclusions regarding this project:

1. The use of original independent variables without any feature engineering resulted in an RMSE of 3.55.
2. Training the model on a dependent variable with a limited range of values returns better results than later cutting the range to the required (RMSE decrease by 1.59).
3. Surprisingly, including sales in previous months did not have a significant impact on improving the result (RMSE decrease by 0.1).

# Further opportunities

Due to hardware and time limitations, two interesting solutions were abandoned, which could measurably affect further improvement of results:

1. Generating new item IDs based on similarities between names ("stringdist" library) because the resulting matrix consisted of 22170^2 observations.
2. A combination of all items in stores was not created for each month because the number of repetitions was nearly 43 million. This would help approximate the distribution of the training set to the test set, in which new items and items-shops pair appear.

In addition, there is still room for:

1. The use of other algorithms such as long short-term memory (type of neural network), hierarchical time series or ensemble modeling.
2. Optimization of model hyperparameters.
3. Further feature engineering.

---

<center> <font size = "2"> <i>
Visualizations were made in `r round(difftime(endTime, startTime, units = "secs"))` s.

Data processing and machine learning were done in `r round(difftime(Sys.time(), endTime, units = "hours"))` hours.
<center> </font> </i>